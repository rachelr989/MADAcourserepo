---
title: "Fitting Exercise"
author: "Rachel Robertson"
date: "3/1/24"
output: html_document
---
# Model Fitting Exercise
## Data Cleaning
First, I will open the libraries I need for this project.
```{r}
library(tidymodels) # modeling and fitting data
library(readr) # reading and importing data
library(broom.mixed) # converting Bayesian models to tidy tibbles
library(dotwhisker) # visualizing regression results
library(ggplot2) # producing graphs and figures to visualize results
library(here) # creating relative pathways to files
library(dplyr) # manipulating and cleaning data frame
library(psych) # create summary tables
library(tidyverse) # creating tidy tables
library(corrplot) # creating correlation plots
```

Next, I will read the csv file to import the Mavoglurant data into R.
```{r}
trial_data <- read_csv(here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"), show_col_types = FALSE) # read csv in the relative path to the fitting exercise folder, without showing column types
str(trial_data) # examine structure of data in the trial_data frame
```

I will go ahead and convert the columns for subject ID, Compartment number, Event ID, occasion, dose amount keyword, and sex to factor variables.
```{r}
trial_data <- trial_data %>%
  mutate(across(c("ID", "CMT", "EVID", "AMT", "OCC", "SEX", "RACE"), as.factor))
str(trial_data)
```

I want to create a line plot showing a dosage over time for each individual in the data set, that is stratified by total dose amount.
```{r}
ggplot(trial_data, aes(x = TIME, y = DV, group = ID, color = DOSE)) + 
  geom_line() +
  labs(title = "Mavoglurant dose over time by individual and stratified by dose", y= "Dosage Variable", x = "Time in hours")
```

Because the value of 2 for OCC (or occasion) reflects 2 dosages, and we are only examining cases that are given 1 dose, we will drop all levels= '2' of the factor variable, OCC.
```{r}
trial_data <- droplevels(trial_data[!trial_data$OCC == '2',]) # drop rows where the level value is '2'
str(trial_data) # examine dataframe structure
levels(trial_data$OCC) # examine levels of the OCC variable
```

All factor levels of OCC except for '1', were dropped. Next, we will drop all rows with the numeric value for TIME = '0'
```{r}
trial_data2 <-trial_data[!(trial_data$TIME %in% 0),] # exclude all rows where time has a value fo zero
summary(trial_data2$TIME) # check the minimum avlue for TIME
```

Now we will use dplyr to find the sum of dosage values (DV) for each individual (ID).
```{r}
sum_DV <- trial_data2 %>% # create data frame for the sum_DV
  group_by(ID) %>% # group by individual 
  summarise( Y = sum(DV)) # make a summary column of DV using dpylr
str(sum_DV) # check structure of sumDV
```

Y, as the sum of DV, was found in a new data frame called sum_DV. We will get a produce for values where TIME=0 and join the data frames.
```{r}
trial_data3 <- trial_data %>%
  filter(TIME == 0) # filter for values of TIME = '0' using dpylr
str(trial_data3) # check structure of new df
trial_data4 <- merge(x = trial_data3, y = sum_DV, by = "ID", all = TRUE) # outer join the data frame to preserve all rows and join them by the ID variable
str(trial_data4) # check structure of joined df
```

Because I have already converted the variables, SEX and RACE, to factors, I will not repeat this step. I will delete the unnecessary columns of my final data frame to only include the variables: Y, DOSE, AGE, SEX, RACE, WT, and HT.
```{r}
trial_data4 <- trial_data4 %>%
  select("Y", "DOSE", "AGE", "SEX", "RACE", "WT", "HT") # select only the aforementioned variables
str(trial_data4) # check data structure
summary(trial_data4)
```

I will now check for any missing values.
```{r}
which(is.na(trial_data4)) # find location of missing values
sum(is.na(trial_data4)) # count total missing values 
```
No missing variables will found, so I will not continue to clean the data and move onto exploratory data analysis. 
## Exploratory Data Analysis 
### Summary Table 
I will create a summary table of the data frame in R by using the describe() function from the "psych" package.
```{r}
table1 <- describe(trial_data4[ , c("Y", "DOSE", "AGE","WT", "HT")],fast=TRUE) # summary stats of cleaned data for the numeric columns
print(table1)
```

In this summary table, we see the numeric variables. They all have a relatively small SE and reasonable min and max values. The range sumDV (Y) is large, but this is likely because this is the sum of the concentration administered to each individual. Now I will bring in the factor variables. I want to stratify the Y variable and dose by AGE, RACE, AGE, WT and HT. I will be using the tidyverse and gtsummary (which I found on stackoverflow) packages to produce this table.
```{r}
table2 <- trial_data4 %>%
  group_by(SEX, RACE) %>%
  summarise(
    mean_Y = mean(Y),
    mean_DOSE = mean(DOSE),
    mean_AGE = mean(AGE),
    sd_Y = sd(Y),
    sd_DOSE = sd(DOSE),
    sd_AGE = sd(AGE),
  )
print(table2)
```
The means of Y and dosage seam to be similar, and within the range specified above with a couple exceptions. SEX(1) and RACE(7) has a lower mean Y. There is also a high DOSE value for SEX(2) RACE(7). This value of 50 is the max, so either all of the values here must be 50.
The mean age for race and sex also varies quite a bit.
It is also strange the sd for dose for SEX(2) RACE(2) is 0.
### Data Distribution
#### Scatter Plots
To examine these strange summary stats in the table in more detail, we will visualize the data distribution using ggplot2.
First, I will produce some scatterplots to see if there are any associations or if the data is randomly distributed
```{r}
# Y versus AGE scatter plot by SEX
plot1 <- ggplot(trial_data4, aes(x = AGE, y = Y, color = SEX)) +
  geom_point() +
  labs(x = "AGE", y = "Y", title = "Scatter Plot of Y versus Age by Sex") +
  theme_minimal()
print(plot1)
# Y versus AGE scatter plot by RACE
plot2 <- ggplot(trial_data4, aes(x = AGE, y = Y, color = RACE)) +
  geom_point() +
  labs(x = "AGE", y = "Y", title = "Scatter Plot of Y versus Age by Race") +
  theme_minimal()
print(plot2)
# WT versus HT scatter plot by Sex
plot3 <- ggplot(trial_data4, aes(x = HT, y = WT, color = SEX)) +
  geom_point() +
  labs(x = "Height", y = "Weight", title = "Scatter Plot of Weight versus Height by Sex") +
  theme_minimal()
print(plot3)
# WT versus HT scatter plot by Race
plot4 <- ggplot(trial_data4, aes(x = HT, y = WT, color = RACE)) +
  geom_point() +
  labs(x = "Height", y = "Weight", title = "Scatter Plot of Weight versus Height by Race") +
  theme_minimal()
print(plot4)
```
The distribution of age seems and Y seem to be fairly random, with the exception of an outlier in the Y variable that is extremely high. 
I will leave this outlier as there is no indication of whether this holds importance (we don't know the definition of many of these variables).
The distribution of Sex for Y and Age values seems to be a higher age for sex 2, but random for the Y variable.
In general, Race seems fairly random for Age and Y, except Race 7 seems to also predominantly be higher ages.
Weight and height show a slight trend, as they normally do, without any extreme outliers. The sex distribution of height and weight also makes sense as females tend to be shorter and weight less than males. For this reason, we might assume that sex 2 is female, but we should not make assumptions.
Races 1, 2, and 7 seem to be randomly distributed for weight and height. However, race 88 seems to be  on the lower end for both height and weight, which may indicate some sex correlation with race.
#### Box Plots
Now I will use box plots to examine the distribution of Dose and Y stratified by the two factor variables, Race and Sex.
```{r}
# Distribution of Y stratified by SEX and RACE
plot6 <- trial_data4 %>% 
  ggplot(aes(x=RACE, y=Y, color = SEX)) +  
  geom_boxplot() +
  labs(x = "RACE", y = "Drug concentration", title = "Race and drug concentraiton distribution, stratified by sex") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))  # Rotate x-axis text by 90 degrees
plot(plot6)
# Distribution of DOSE stratified by SEX and RACE
plot7 <- trial_data4 %>% 
  ggplot(aes(x=RACE, y=DOSE, color = SEX)) +  
  geom_boxplot() +
  labs(x = "RACE", y = "Drug concentration", title = "Race and Dose distribution, stratified by sex") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))  # Rotate x-axis text by 90 degrees
plot(plot7)
```
There are two outliers for Y, in race 1. Each outlier is of a different sex, so sex might not impact the outliers.There are only 2 values for race 7, making their distribution look wonky.
In general, sex 1 has a higher Y and dose than sex 2.
No outliers is dose were seen as there are only 3 options for dose. Again, because there are two values in race 7, the distribution is wonky.
I will choose to keep Race 7, because I do not know if this represents an important minority group, despite there only being 2 values.
### Correlation Plots
Now I will create a correlation plot of the numeric variables.
To do this, I asked ChatGPT to provide me with the functions used to make correlation plots for numeric variables.
```{r}
numeric_data <- trial_data4[, sapply(trial_data4, is.numeric)]
correlation_matrix <- cor(numeric_data) # Calculating correlation matrix
corrplot(correlation_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45) # Creating correlation plot
```
From the correlation plot, it seems that weight and height, and Y and Dose are most highly correlated. Age and height are moderately negatively correlated, as well as weight and Y are slightly negatively correlated.
## Model Fitting
First, I will fit a linear model to the outcome, Y, using the predictor of DOSE.
```{r}
set.seed(333)
data_split <- trial_data4 %>% 
  initial_split(prop = 3/4) #split the data so that 3/4 is in the training set and 1/4 in the reference set
train_data <- training(data_split) # make the training data object
test_data  <- testing(data_split) # make testing data object

trial_linear_fit_1 <- 
  recipe(Y ~ DOSE, data = train_data) 

lm_mod <- linear_reg() %>% # make object called lm_mod for linear_regression function
  set_engine("lm") %>% # set linear regression engine to lm
  set_mode("regression") # set to regression mode

trials_workflow1 <- workflow() %>% #create a workflow
    add_model(lm_mod) %>% #apply the linear regression model
  add_recipe(trial_linear_fit_1) #then, apply the recipe

lm_fit <- 
  lm_mod %>% 
  fit(Y ~ DOSE, data = trial_data4) # fit Y to dose 
tidy(lm_fit) # produce tibble for linear regression fit
```
The intercept has a relatively large standard error, while the DOSE regression estimate has a more reasonable SE.
This linear regression reflects a positive trend between dose and Y with a slope of 58.2.
I will now use this model to predict the trial data. This will allow me to calculate the predictive power using the RMSE and R-squared metrics.
```{r}
trial_fit1 <- 
  trials_workflow1 %>% 
  fit(data = train_data) # shows model to fit the data used from training from the aforementioned workflow
   predict(trial_fit1, test_data) #predict the outcome Y
trial_aug <- 
  augment(trial_fit1, test_data)
trial_aug %>%
  select("Y", "DOSE", "AGE", "SEX", "RACE", "WT", "HT")
names(trial_aug) # check the name for the predicted column

eval_metrics <- metric_set(rmse, rsq) # use yardstick to select multiple regression metrics (rmse and r squared)

eval_metrics(data = trial_aug,
             truth = Y,
             estimate = .pred) %>% 
  select(-2) # Evaluate RMSE, R2 based on the results
```
The RMSE is very large and rsq is close to 0.5 which shows that the predictive model is not a very good fit for the data.
Next, I will fit a linear model to the outcome, Y, using all of the predictors.
```{r}
trial_linear_fit_2 <- 
  recipe(Y ~ ., data = train_data) 

lm_mod2 <- linear_reg() %>% # make object called lm_mod for linear_regression function
  set_engine("lm") %>% # set linear regression engine to lm
  set_mode("regression") # set to regression mode

trials_workflow2 <- workflow() %>% #create a workflow
    add_model(lm_mod2) %>% #apply the linear regression model
  add_recipe(trial_linear_fit_2) #then, apply the recipe

lm_fit2 <- 
  lm_mod2 %>% 
  fit(Y ~ ., data = trial_data4) # fit Y to dose 
tidy(lm_fit2) # produce tibble for linear regression fit
```
The standard error for the intercept, SEX2, RACE2, Race7, and HT are extremely large. This may be in part due to the large standard error of the original Y (outcome) variable. Dose, Age and Race2 have a positive trend with Y while the variables Sex2, Race7, Race88, WT, and HT have a negative trend with Y. 
```{r}
trial_fit2 <- 
  trials_workflow2 %>% 
  fit(data = train_data) # shows model to fit the data sued from training from the aforementioned workflow
   predict(trial_fit2, test_data) #predict the outcome Y

trial_aug <- 
  augment(trial_fit2, test_data)
trial_aug %>%
  select("Y", "DOSE", "AGE", "SEX", "RACE", "WT", "HT")

eval_metrics <- metric_set(rmse, rsq) # use yardstick to select multiple regression metrics (rmse and r squared)

eval_metrics(data = trial_aug,
             truth = Y,
             estimate = .pred) %>% 
  select(-2) # Evaluate RMSE, R2 based on the results
```
The R squared value for the model including all of the predictors is slightly better than the previous model (at around 0.6), but still NOT good. The rmse has not changed much, reflecting a large impact of outliers on the standard error of the predictions in the linear model.

Now we will make a model with SEX to practice fitting with a categorical outcome variable. We will fit a logistic model to the SEX outcome using the predictor, DOSE.
```{r}
trial_logreg_rec <- 
  recipe(SEX ~ DOSE, data = train_data) %>% # create recipe for logistic regression model using SEX as outcome
  prep() # Prepare the recipe

lr_mod <- logistic_reg() %>% # make object called lr_mod for logistic regression function
  set_engine("glm") # set logistic regression engine to glm

lr_trials_workflow <- workflow() %>% # create a workflow
  add_model(lr_mod) %>% # apply the logistic regression model
  add_recipe(trial_logreg_rec) # then, apply the recipe

lr_fit <- 
  lr_mod %>% 
  fit(SEX ~ DOSE, data = trial_data4) # fit Y to dose 
tidy(lr_fit) # produce tibble for log regression fit
```
There is a relatively high standard error for both the dose and intercept estimates. Both estimates are negative, meaning there is a negative trend between dose and sex.

We will now compute accuracy and ROC-AUC for this model.
```{r}
lr_trial_fit <- 
  lr_trials_workflow %>% 
  fit(data = train_data)

predict(lr_trial_fit, new_data = test_data)

lr_trial_aug <- 
  augment(lr_trial_fit, test_data)

lr_trial_aug %>%
  select("Y", "DOSE", "AGE", "SEX", "RACE", "WT", "HT")

names(lr_trial_aug)

ROC1 <- lr_trial_aug %>% 
  roc_auc(truth = SEX, .pred_1)
print(ROC1)
ROC2 <- lr_trial_aug %>% 
  roc_auc(truth = SEX, .pred_2)
print(ROC2)
```
The AUC metric for the first predictor is 0.62 and for the second is 0.38. They are both the same magnitude away from 0.5. This means that both predictions are equally as good, but neither prediction is great because it they are not very close to 1 or 0. This means that this model doesn't have great predictive power either.

Next, we will fit a logistic model to SEX using all of the other predictors.
```{r}
trial_logreg_rec2 <- 
  recipe(SEX ~ ., data = train_data) %>% # create recipe for logistic regression model using SEX as outcome
  prep() # Prepare the recipe

lr_mod2 <- logistic_reg() %>% # make object called lr_mod for logistic regression function
  set_engine("glm") # set logistic regression engine to glm

lr_trials_workflow2 <- workflow() %>% # create a workflow
  add_model(lr_mod2) %>% # apply the logistic regression model
  add_recipe(trial_logreg_rec2) # then, apply the recipe

lr_fit2 <- 
  lr_mod2 %>% 
  fit(SEX ~ ., data = trial_data4) # fit Y to dose 
tidy(lr_fit2) # produce tibble for log regression fit
```
The intercept now has a much higher estimate, of 60 and a smaller SE than the previous logistic regression model. Almost all of the variables have a negative trends with the Sex variables, except for Race7. In addition, all of the estimates for the predictor variables versus Sex (outcome) have a very high SE, meaning that they have a high variability.

We will also compute accuracy and ROC-AUC for this model.
```{r}
lr_trial_fit2 <- 
  lr_trials_workflow2 %>% 
  fit(data = train_data)

predict(lr_trial_fit2, new_data = test_data)

lr_trial_aug2 <- 
  augment(lr_trial_fit2, test_data)

lr_trial_aug2 %>%
  select("Y", "DOSE", "AGE", "SEX", "RACE", "WT", "HT")

names(lr_trial_aug2)

ROC1_2 <- lr_trial_aug2 %>% 
  roc_auc(truth = SEX, .pred_1)
print(ROC1)
ROC2_2 <- lr_trial_aug %>% 
  roc_auc(truth = SEX, .pred_2)
print(ROC2)
```
With a AUC of 0.62 and 0.38, both predictions are equally good, but also both not the greatest. AUC values closer to 0.5 means the model is more near a random model versus an AUC close to 0 or 1, which might show good predictive power.

# Part 2
## Model Improvement
For this exercise, we will be using the data previously used to fit our linear and logistic models to create new models and improve those models.
We will be using tidymodels and the packages previously opened for the first part of this exercise.
We will start this exercise by eliminating the variable entitled RACE because it has values (7 and 88) with too few observations, and which might be acting as outliers.
```{r}
trial_data5 <- subset(trial_data4, select = -RACE) # Use subset to column to drop 
str(trial_data5) # check structure of new df
```
Next, we will set a seed for reproducibility. And split the data with 75% in the training set and 25% in the testing set.
```{r}
rngseed <- 1234
set.seed(rngseed) # set seed according to the variable rngseed (which equals 1234)
# Data Splitting
library(rsample)
trial5_split <- initial_split(trial_data5, prop = 3/4)# create new data frame with the 3/4 split
train_data2 <- training(trial5_split) # create data frame for training data
test_data2  <- testing(trial5_split) # create data frame for testing data
```
Next, I will produce a linear model with just one predictor, DOSE, the outcome, or the Y variable. I will then produce a full model including all of the predictors: DOSE, AGE, SEX, WT, and HT.
```{r}
# Single Predictor Model (Y v. DOSE)
single_rec <- 
  recipe(Y ~ DOSE, data = train_data2) # specify the recipe by putting the formula for the null linear model

single_linreg <- linear_reg() %>% # make object called null_linreg for linear_regression function
  set_engine("lm") %>% # set linear regression engine to lm
  set_mode("regression") # set to regression mode

workflow_single <- workflow() %>% #create a workflow for the linear model called workflow_null
    add_model(single_linreg) %>% #apply the linear regression model
  add_recipe(single_rec) #then, apply the recipe

single_lmfit <- 
  workflow_single %>% 
  fit(data = train_data2) # fit Y to dose using ONLY the training data
tidy(single_lmfit) # produce tibble for linear regression fit
```


```{r}
# Full model (Y v. all predictors)
full_rec <- 
  recipe(Y ~ ., data = train_data2) # specify the recipe by putting the formula for the linear model with all the predictors

full_linreg <- linear_reg() %>% # make object called full_linreg for linear_regression function
  set_engine("lm") %>% # Use the same linear regression engine as the null model
  set_mode("regression") # and also, set to regression mode

workflow_full <- workflow() %>% #create a workflow for the full linear model called workflow_full
    add_model(full_linreg) %>% #apply the new linear regression model
  add_recipe(full_rec) #then, apply the recipe

full_lmfit <- 
   workflow_full %>% # add the workflow to the fit
  fit(data = train_data2) # fit Y to dose using ONLY the training data
tidy(full_lmfit) # produce tibble to organize the resulting linear regression fit
```
### Using RMSE to evaluate linear models
Now I will compute the predictions that are made by both the singular predictor model and the full model. RMSE will be calculated using the observed versus the predicted values. 
Lastly, we will compare these RMSE values to the RMSE of the null model as fitted using the null_model() function. 

#Edited by Taylor Glass
I noticed that the reason your RMSE values were not as expected is because you were using the test_data2 instead of the train_data2. This entire process should be using the train_data2 only, so I made those changes. The RMSE values are now 948, 702, and 627 respectively. 
```{r}
# Make predictions with the single predictor linear model
predict(single_lmfit, train_data2) #predict the outcome Y from the single predictor linear model (Y v. dose)
single_aug <- 
  augment(single_lmfit, train_data2) # add columns of interest to new data frame
single_aug %>%
  select("Y", "DOSE") # select columns to add to the data frame
names(single_aug) # check the names

# Calculate the RMSE of the single predictor model
rmse_sing <- rmse(data = single_aug,
             truth = Y,
             estimate = .pred)
  print(rmse_sing) # Evaluate RMSE based on the results

# Make predictions with the full predictor linear model
predict(full_lmfit, train_data2) #predict the outcome Y from the FULL predictor linear model (Y v. all predictors)
full_aug <- 
  augment(full_lmfit, train_data2) # add predictions to new data frame
full_aug %>%
  select("Y", "DOSE", "AGE", "SEX", "WT", "HT") # select columns to add to the data frame
names(full_aug) # check the names for the columns
str(full_aug)

# Calculate the RMSE of the full model
rmse_full <- rmse(data = full_aug,
             truth = Y,
             estimate = .pred)
  print(rmse_full) # Evaluate RMSE based on the results

# Make predictions using a null model
blank_model <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("regression") # use the null_model function and the parsnip engine to create a blank model

blank_rec <-
  recipe(Y ~ 1, data = train_data2) # produce recipe for blank model

workflow_blank <- workflow() %>% #create a workflow for the null model called workflow_blank
    add_model(blank_model) %>% #apply the new null model
  add_recipe(blank_rec) #then, apply the recipe

blank_fit <- 
   workflow_blank %>% # add the workflow to the fit
  fit(data = train_data2) # fit Y to dose using ONLY the training data
tidy(blank_fit) # produce tibble to organize the resulting linear regression fit

predict(blank_fit, train_data2) #predict the outcome Y from the BLANK predictor linear model
blank_aug <- 
  augment(blank_fit, train_data2) # add predictions to new data frame
blank_aug %>%
  select("Y", "DOSE", "AGE", "SEX", "WT", "HT") # select columns to add to the data frame
names(blank_aug) # check the names for the columns

# Calculate the RMSE of the blank model
rmse_blank <- rmse(data = blank_aug,
             truth = Y,
             estimate = .pred)
  print(rmse_blank) # Evaluate RMSE based on the results
```
The RMSE for the single predictor model, full model, and null model are 560, 520, and 993 respectively. This is not equal to the values found by Dr. Handel, however I did set the seed in the initial step.
The RMSE of both the full and single predictor models are similar, with a difference of 60, while the RMSE for the null model is much greater than both models including predictors. The null model has an RMSE that with a difference of 433 and 393 for the single predictor and full models, respectively.
This reflects that both models that contain predictors are better at predicting values based on the original observations, than the null model is at predicting the observations. This means that both models including predictors are better than random models, and capture some trend using linear regression.
### Using cross validation to evaluate linear models
I will now use cross validation to evaluate the linear models and null model that were produced. Cross validation is a re-sampling method to measure model performance while accounting for the variance-bias trade off. Here, we will use a 10-fold cross validation method, which compares the train and test data 10 times (using different subsamples) using RMSE to check for consistancy in model predictions.
I start by specifying the workflow for the cross validation method. Then I will use the fit_resamples function to perform the CV fit. I will use RMSE to compare each models' CV fit.
```{r}
# Single Predictor Linear Model Cross Validation
folds <- vfold_cv(trial_data5, v = 10)
single_cv_wf <- 
  workflow() %>% #define an object for the cv workflow 
  add_model(single_linreg) %>% # add in the previous linear model
  add_recipe(single_rec) # specify the formula for the first linear model

single_fit_cv <- 
  single_cv_wf %>% # apply cv workflow to an object for the cv fit
  fit_resamples(folds) # specify the number of folds for the cv
print(single_fit_cv)

# Examine rmse of the cv fit
rmse_values1 <- single_fit_cv %>%
  collect_metrics() %>% # get the metrics for all of the re-sampled folds
  filter(.metric == "rmse") # filter by the rmse values only
print(rmse_values1)

# Full Linear Model Cross Validation
full_cv_wf <- 
  workflow() %>% #define an object for the cv workflow 
  add_model(full_linreg) %>% # add in the previous linear model
  add_recipe(full_rec) # specify the formula for the first linear model

full_fit_cv <- 
  full_cv_wf %>% # apply cv workflow to an object for the cv fit
  fit_resamples(folds) # specify the number of folds for the cv

# Examine rmse of the cv fit
rmse_values2 <- full_fit_cv %>%
  collect_metrics() %>% # get the metrics for all of the re-sampled folds
  filter(.metric == "rmse") # filter by the rmse values only
print(rmse_values2)

# Null Model Cross Validation
null_cv_wf <- 
  workflow() %>% #define an object for the cv workflow 
  add_model(blank_model) %>% # add blank model in
  add_recipe(blank_rec)

null_fit_cv <- 
  null_cv_wf %>% # apply cv workflow to an object for the cv fit
  fit_resamples(folds) # specify the number of folds for the cv

# Examine rmse of the cv fit
rmse_values3 <- null_fit_cv %>%
  collect_metrics() %>% # get the metrics for all of the re-sampled folds
  filter(.metric == "rmse") # filter by the rmse values only
print(rmse_values3)
```
The mean RMSE for both models including predictors greatly increased, while the mean RMSE for the null model stayed nearly the same.
For the model including a single predictor, the RMSE increased by 99 and the full model RMSE increased by 97. Both models had an rmse value increase by approximately the same magnitude.
The standard error for all three models (including the null) are very similar (58, 53, and 55), which demonstrates consistency between models despite varying numbers of predictors being used.

Lastly, I will run the code again with a different seed to see if the cross validation and rmse values change.
```{r}
set.seed(333)
# Single Predictor Linear Model Cross Validation
folds <- vfold_cv(trial_data5, v = 10)
single_cv_wf <- 
  workflow() %>% #define an object for the cv workflow 
  add_model(single_linreg) %>% # add in the previous linear model
  add_recipe(single_rec) # specify the formula for the first linear model

single_fit_cv <- 
  single_cv_wf %>% # apply cv workflow to an object for the cv fit
  fit_resamples(folds) # specify the number of folds for the cv
print(single_fit_cv)

# Examine rmse of the cv fit
rmse_values1 <- single_fit_cv %>%
  collect_metrics() %>% # get the metrics for all of the re-sampled folds
  filter(.metric == "rmse") # filter by the rmse values only
print(rmse_values1)

# Full Linear Model Cross Validation
full_cv_wf <- 
  workflow() %>% #define an object for the cv workflow 
  add_model(full_linreg) %>% # add in the previous linear model
  add_recipe(full_rec) # specify the formula for the first linear model

full_fit_cv <- 
  full_cv_wf %>% # apply cv workflow to an object for the cv fit
  fit_resamples(folds) # specify the number of folds for the cv

# Examine rmse of the cv fit
rmse_values2 <- full_fit_cv %>%
  collect_metrics() %>% # get the metrics for all of the re-sampled folds
  filter(.metric == "rmse") # filter by the rmse values only
print(rmse_values2)

# Null Model Cross Validation
null_cv_wf <- 
  workflow() %>% #define an object for the cv workflow 
  add_model(blank_model) %>% # add blank model in
  add_recipe(blank_rec)

null_fit_cv <- 
  null_cv_wf %>% # apply cv workflow to an object for the cv fit
  fit_resamples(folds) # specify the number of folds for the cv

# Examine rmse of the cv fit
rmse_values3 <- null_fit_cv %>%
  collect_metrics() %>% # get the metrics for all of the re-sampled folds
  filter(.metric == "rmse") # filter by the rmse values only
print(rmse_values3)
```
The RMSE values for this cross validation are slightly lower than the previous; 669 for the single predictor model, 606 for the full model and 953 for the null model. The standard errors for all three fits have greatly decreased to 28, 48, and 41 respectively. Those rmse values with lower se values are likely a better reflection of model performance than the rmse values in the previous cv (with higher se).

# This part added by Taylor Glass. 
## Model Predictions
I will plot the observed and predicted values from the first 3 models with all of the training data. I will create a data frame with these values first and add a label to indicate the model. 
```{r}
# set seed for reproducibility
set.seed(rngseed)

# create data frame with the observed values and 3 sets of predicted values 
plotdata <- data.frame(
  observed = c(train_data2$Y), 
  predicted_null = c(blank_aug$.pred), 
  predicted_model1 = c(single_aug$.pred), 
  predicted_model2 = c(full_aug$.pred), 
  model = rep(c("null model", "model 1", "model 2"), each = nrow(train_data2))) # add label indicating the model
plotdata
```

I will use the new data frame to create a visualization. 
```{r}
# create a visual representation
ggplot(plotdata, aes(x = observed)) +
  geom_point(aes(y = predicted_null, color = "null model"), shape = 1) +
  geom_point(aes(y = predicted_model1, color = "model 1"), shape = 2) +
  geom_point(aes(y = predicted_model2, color = "model 2"), shape = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + #add the 45 degree line
  xlim(0, 5000) + #limit the x-axis values
  ylim(0, 5000) + #limit the y-axis values
  labs(x = "Observed Values", y = "Predicted Values")
```
The predictions from the null model are a horizontal line because each prediction is just the mean observation. The predictions from model 1 with the `DOSE` variable only are shown in three horizontal lines, which makes sense because the `DOSE` variable has 3 discrete values. The predicted values should be the same for each of those 3 dosage values because there are only 3 original values to predict. The predictions for model 2 with all variables is the best and shows a lot of scatter that seems to follow a pattern along the 45 degree line. 

To determine if there is a pattern in the residuals, I will plot the predicted values against the residuals. The formula for finding the residual is just (predicted-observed). I will create a new data frame with the predicted values and residuals first. 
```{r}
# create data frame with predictions and residuals for model 2
plotresiduals <- data.frame(
  predicted_model2 = c(full_aug$.pred), 
  residuals = c(full_aug$.pred - full_aug$Y)) #calculate residuals by predicted - observed
plotresiduals
```

I will plot the new residual values to look for any patterns. 
```{r}
# plot predictions versus residuals for model 2
ggplot(plotresiduals, aes(x=predicted_model2, y=residuals)) + 
  geom_point() + 
  geom_abline(slope = 0, intercept = 0, color = "pink", size = 1.5) + #add straight line at 0
  ylim(-2000,2000) + #make sure y-axis goes the same amount in positive and negative direction
  labs(x= "Predicted Values", y= "Residuals")
```
As Dr. Handel suggested, I see a pattern in these residuals because there are more and higher negative residuals compared to positive residuals. There are 6 observations less than -1000, but there is only 1 observation more than 1000. The positive residuals are grouped together more, while the negative residuals are more spaced out. This could be because there is important information missing or the model is too simple.

## Model predictions and uncertainty
To determine uncertainty in the predictions, I will use a bootstrap method to sample the data 100 times and fit the model to each set of predictions.
```{r}
# set the seed for reproducibility
set.seed(rngseed)

# create 100 bootstraps with the training data
bootstraps <- bootstraps(train_data2, times = 100)

# create empty vector to store predictions list 
preds_bs <- vector("list", length = length(bootstraps))

# write a loop to fit the model to each bootstrap and make predictions
for (i in 1:length(bootstraps)) {
  
  bootstrap_sample <- analysis(bootstraps$splits[[i]])  # isolate the bootstrap sample
  
  model <- lm(Y ~ DOSE + AGE + HT + WT + SEX, data = bootstrap_sample) # fit the model using the bootstrap sample
  
  predictions <- predict(model, newdata = train_data2) # make predictions with the training data
  
  preds_bs[[i]] <- predictions # store predictions in the empty vector
}

# find an individual bootstrap sample 
sample <- analysis(bootstraps$splits[[i]])
sample
```

Now that I have all of my predictions stored in a vector, I need to convert it to an array, so I can compute the median and 95% confidence intervals for each one. 
```{r}
# create an empty array to store the predictions
num_samples <- length(preds_bs)
num_datapoints <- length(preds_bs[[1]])  
preds_array <- array(NA, dim = c(num_samples, num_datapoints))

# fill the array with predictions from bootstrappping
for (i in 1:num_samples) {
  preds_array[i,] <- unlist(preds_bs[[i]])
}

# find the median and 95% confidence intervals of each prediction
preds <- preds_array %>%
          apply(2, quantile,  c(0.05, 0.5, 0.95)) %>%
          t()
```

Finally, I will graph the observed values on the x-axis and point estimates from the training data on the y-axis. I will add the median and confidence intervals for the predictions from the bootstrap sampling to the y-axis as well.  
```{r}
# create a data frame with all the necessary variables
finaldata <- data.frame(
  observed = c(train_data2$Y), 
  point_estimates = c(full_aug$.pred),
  medians = preds[, 2],
  lower_bound = preds[, 1],
  upper_bound = preds[, 3]
)

# create visualization with all 5 variables 
ggplot(finaldata, aes(x = observed, y = point_estimates)) +
  geom_point(color = "black") +  
  geom_point(aes(y = medians), color = "green") + 
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.1, color = "blue") + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # add 45 degree line
  xlim(0, 5000) +
  ylim(0, 5000) +
  labs(x = "Observed Values", y = "Predicted Values")
```
This plot shows that our model does a good job of predicted the observed values because most of the point estimates lie around the 45 degree line. I think some of the variation shown in the bootstrap estimates is accounted for by the fact that the `DOSE` variable has 3 discrete values, so those predicted values will follow 3 horizontal lines as shown in a previous graph. All of the median values from the bootstrap sampling, shown in green, are father away from the 45 degree line than the actual predicted values, shown in black. There are many instances where the confidence interval from the bootstrapping estimate does not contain the point estimate, but some of the confidence intervals contain the point estimates, especially where the point estimates are clustered. I think this graph is evidence that our model is a decent option for this data because the general pattern of point estimates follows the expected 45 degree line. 
# Rachel continued this section
Now we will use the fitted full model to make predictions for the test data. This will allow us to assess the generalizability of the full model that was fitted.
I will start by refitting the full linear regression model to both the training and testing data, using the seed set for the previous analysis.
```{r}
# predictions based on testing data
test_pred_aug <- augment(full_lmfit, test_data2) %>% # use augment to make predictions
  select(Y, .pred) %>% # select columns for plotting
  rename(test_pred = .pred) # change the name of the predictions column

# predictions based on training data
train_pred_aug <- augment(full_lmfit, train_data2) %>%
  select(Y, .pred) %>%
  rename(train_pred = .pred)

# combine prediction columns into one data frame
combined_df <- full_join(trial_data5, test_pred_aug, by = "Y") %>%
  full_join(train_pred_aug, by = "Y") # use full join to retain all values for Y and trial_data 5 for all of the original Y values
str(combined_df) # check the structure
```
Now, we will plot these two model predictions against the original Y observations to visualize if the testing data produced predictions similar to the training data (and if the model in generalizable).
```{r}
# grap both predictions
train_test_plot <- ggplot(combined_df, aes(x = Y)) +
  geom_point(aes(y = test_pred, color = "testing predictions"), shape = 1) +
  geom_point(aes(y = train_pred, color = "training predicitons"), shape = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + #add the 45 degree line
  xlim(0, 5000) + #limit the x-axis values
  ylim(0, 5000) + #limit the y-axis values
  labs(x = "Observed Y Values", y = "Predicted Values for Train and Test data")
print(train_test_plot)
```
The chart above displays that the training data predictions and testing data predictions follow the same pattern. This means that the model captures the same trends that it did on the testing data as when it was fitted using the training data. This means that the model is generalizable when presented with "new" data. However, the pattern in both sets of predictions shows that a trend was not captured. Even though the model is generalizable, it is also under-fitted and may be too flexible. This means that more tuning, additional predictors, or a larger sample size, will help the model fit the unrepresented trend. 

## Conclusions
>We want to make sure that any model we have performs better than the null model. Is that the case?

According to the RMSE computations for the single predictor model, full model, and null model, which are 560, 520, and 993 respectively, the models including any predictor are both better than the null model. However, we see in the plot of the predicted values of both models versus the null, that the model including the single predictor is not much better than the null model. The null model shows a single horizontal line of predictions, while the single predictor model shows three horizontal lines of predictions. The points in the single predictor model follow the trend (of the 45 degree line) slightly better than the null model, but with only three possible values for the predictor (DOSE), the overall trends of Y are not captured. When you add in all of the predictors, this allows more trends to be captured. This is shown in the graph of the full model predictions, where the points more closely follow the 45 degree trend line.

>Does model 1 with only dose in the model improve results over the null model? Do the results make sense? Would you consider this model “usable” for any real purpose?

Model 1 with only DOSE as a predictor slightly improves the results compared to the null model. As discussed before, model 1 has a much lower RMSE value and follows the trend line slightly better than the null model. However, because there are only 3 options for dose, it leaves out many trends contributing the the distribution of Y. This is why the single predictor model would not be usable for any real purpose. Although one could use it to predict the Y given DOSE, we learn that many other predictors impact the trends in Y and DOSE might not be the best predictor.

>Does model 2 with all predictors further improve results? Do the results make sense? Would you consider this model “usable” for any real purpose?

Model 2 with all of the predictors does improve the predictions compared to model 1 and the null model. First, the RMSE is lower for the full model than all of the others. Secondly, in the graph, you can see a clear cloud of points following the 45 degree trend line. The full model predictions follow the trend line more closely than all of the other model predictions, showing that the full model fits best. However, there is still a pattern, seen in the residual plot, where higher predicted values are more scattered and contain more outliers. This means that the full model still is not able to capture all of the trends contributing to the observed Y values. This model is still more usable for predicting Y given any of the predictors in the model, because it is more accurate. It must be kept in mind that for larger Y values, the predictive power of the full model likely decreases. This means that it can be used to make predictions for small Y values but not larger ones.